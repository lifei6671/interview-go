# 常见基础大模型知识面试题

## 1、大模型中的“微调”是什么

大模型中的“微调”是一种**在预训练基础上继续训练，使模型针对特定任务、特定领域或特定风格表现更好**的技术。

一句话概括：

**微调就是让通用大模型，变成在某个具体场景上表现更专业的小模型。**

核心点包括：

1. **基于已有模型继续训练**
   避免从零训练的巨大成本，利用原模型的通用知识。

2. **使用小规模、高质量的数据集**
   例如客服对话、金融问答、医疗知识、公司业务语料。

3. **模型仅在部分参数或全部参数上更新（LoRA、Fine-tune、PT）**
   常见方式包括全参数微调、LoRA 微调、Prefix-Tuning 等。

4. **目标是让模型在特定任务上效果更好**
   例如提高客服机器人命中率、让模型生成符合企业风格的内容、增强模型在专业领域的回答能力。

5. **本质是“定制化”**
   让一个通用大模型变成一个业务专家模型。


## 2、大模型中的“SFT”是什么？

SFT 的全称是 **Supervised Fine-Tuning（监督式微调）**，是大模型训练流程中最关键、最常用的一步。

**SFT 是大模型在预训练之后，使用人工标注的高质量示例进行的“监督式微调”。**

它的目标是让模型按照人类意图输出更自然、更符合任务要求的回答。

要点包括：

1. **有监督学习**：使用成对的“输入 → 标准答案”数据（prompt、response）。
2. **让模型学会“该怎么回答”**，从通用模型变成任务模型。
3. **典型用于对话、指令跟随、任务执行等场景**。
4. **常作为 RLHF（强化学习）之前的重要步骤**。

一句话总结：

**SFT 就是用人工示范样例教大模型如何按人类的方式完成任务，是大多数指令模型（如 ChatGPT 类模型）的核心训练步骤。**

## 3、大模型中的“CoT（Chain of Thought）”是什么？

**CoT 是 Chain of Thought 的缩写，指“思维链”提示方式。**

通过让模型输出中间推理过程，而不是直接给答案，可以显著提升模型在**复杂推理、数学、逻辑、规划**等任务上的表现。

关键点：

1. **本质是显式推理**
   让模型把每一个推理步骤写出来，再得到最终结论。

2. **依赖示例（Few-shot CoT）**
   给模型提供带“步骤和答案”的示例，它就会模仿这种推理模式。

3. **提升模型推理能力**
   特别是在数学计算、逻辑推断、程序题、规划路径等链式推理任务中效果很好。

4. **不是 RLHF 或 SFT 的替代，而是 Prompt 技术**
   CoT 可用于推理测试，也常作为数据加入 SFT，让模型具备推理能力。

一句话总结：

**CoT 是一种让模型“说出思考过程”的方法，通过显式推理链条来提升模型的逻辑和推理效果。**

## 4、大模型中的“RAG（Retrieval-Augmented Generation）”是什么

**RAG 是 Retrieval-Augmented Generation，用外部检索系统补充大模型的知识，让模型“先查再答”。**
解决大模型幻觉、知识过时、领域知识不足的问题。

核心流程包含三步：

1. **检索（Retrieval）**
   根据用户问题，从向量数据库或文档库中召回最相关的内容（Embedding 相似度）。

2. **增强（Augment）**
   将检索到的内容与原问题拼接，作为模型新的输入 Prompt。

3. **生成（Generation）**
   大模型基于“问题 + 外部知识”生成更加准确、可引用的回答。

典型适用场景：

* 企业知识库问答
* 文档助手/法律助手/医疗助手
* FAQ 自动化
* 专业领域问答
* 需要实时信息的任务（例如最新文档、公司内部资料）

一句话总结：
**RAG 让模型不依赖记忆，而是通过检索获得事实，确保回答更稳定、更准确、更可控。**

## 5、大模型中的“RL（Reinforcement Learning）”是什么？

**RL 是 Reinforcement Learning 的缩写，即强化学习。**
在大模型训练中，它是让模型通过“奖励函数”不断调整策略，从而更符合人类意图的一种训练方式。

关键特点：

1. **通过奖励优化模型行为**
   不再依赖标准答案，而是通过“好行为给奖励、坏行为给惩罚”来优化模型的输出策略。

2. **常用于人类偏好对齐（Alignment）**
   典型流程是人类先给出模型回答的偏好排序，然后训练一个奖励模型，再用 RL（如 PPO）优化模型。

3. **解决 SFT 的不足**
   SFT 只能让模型“模仿人类示例”，RL 可以进一步强化模型应该做的行为，弱化不希望的行为。

4. **用于提升模型的礼貌性、安全性、稳定性和可控性**
   例如减少幻觉、避免敏感内容、提升回答一致性。

一句话总结：
**RL 是让大模型通过奖励机制学习如何“更像人类希望的那样”回答，是对齐技术中非常关键的一步。**

## 6、大模型中的“GRPO（Generative Rejection Preference Optimization）”是什么？


**GRPO 是 Generative Rejection Preference Optimization，是一种不依赖复杂强化学习（如 PPO），通过“生成-拒绝-偏好优化”来对齐大模型的训练技术。**

核心思想是：

1. **模型生成多个候选回答**
   例如针对一个 prompt 生成 K 个不同的输出。

2. **利用规则、奖励模型或过滤逻辑对回答进行排序或淘汰**
   识别出“好的回答”和“差的回答”。

3. **只保留高质量的人类偏好回答，用监督方式继续训练模型**
   不做真正的 RL，不需要 PPO，也不需要复杂的环境交互。

4. **实现“更简单的偏好优化”**
   既保留了 RLHF“偏好对齐”的优势，又避免 RL 的高成本和不稳定性。

一句话总结：
**GRPO 是一种“用拒绝差答案 + 继续监督训练”来对齐大模型偏好的方法，被认为是比传统 RLHF 更稳、更便宜的下一代偏好优化技术。**

## 7、大模型中的“MOE（Mixture of Experts）”是什么？


**MOE 是 Mixture of Experts，即“专家混合模型”。它通过引入多个专家子模型，并使用路由器在每次前向计算中只激活少数专家，从而实现“大容量、低计算”的模型结构。**

核心机制：

1. **多个专家（Expert）并行存在**
   每个 expert 是一组独立的全连接层或 FFN。

2. **路由器（Router/Gating Network）选择性激活专家**
   输入 token 经过 Router 后，会被分配到最相关的少量专家（如 top-1 或 top-2）。

3. **稀疏激活（Sparse Activation）**
   虽然模型包含成百上千个专家，但每次只使用 1-2 个，因此计算量大幅降低。

4. **达到“参数多，算力少”的效果**
   例如参数量看起来 500B，但训练/推理的 FLOPs 类似 50B。

一句话总结：
**MOE 用“专家路由 + 稀疏激活”实现了超大参数规模与高效计算的统一，是目前扩展大模型能力的重要架构。**

## 8、大模型中的“Scaling（扩展规律）”是什么？

**Scaling（扩展规律）是指：大模型的损失函数、能力和泛化性能，随着参数规模、训练数据量和计算量按特定比例扩大时，会呈现近似幂律（Power Law）的规律性提升。**

也就是说，只要给模型：

* **更多参数（更大的网络）**
* **更多训练数据**
* **更多计算**

其性能就会按照可预测的数学曲线持续提升。

典型特征：

1. **呈幂律关系（Power-Law）**
   损失 L 与规模 N 满足公式：
   [
   L(N) = aN^{-b} + c
   ]

2. **参数、数据、算力必须协同扩展**
   不能只扩大参数或数据，否则会浪费算力。

3. **可根据扩展规律规划下一代模型规模**
   例如 OpenAI、DeepMind、Anthropic 都依赖 scaling law 设计 GPT、Gemini、Claude。

一句话总结：
**Scaling Law 让大模型从“靠经验调参”变成“按数学公式设计模型”，并推动了 GPT-3 以后的整个大模型时代。**
