# 常见基础大模型知识面试题

## 1、大模型中的“微调”是什么

大模型中的“微调”是一种**在预训练基础上继续训练，使模型针对特定任务、特定领域或特定风格表现更好**的技术。

一句话概括：

**微调就是让通用大模型，变成在某个具体场景上表现更专业的小模型。**

核心点包括：

1. **基于已有模型继续训练**
   避免从零训练的巨大成本，利用原模型的通用知识。

2. **使用小规模、高质量的数据集**
   例如客服对话、金融问答、医疗知识、公司业务语料。

3. **模型仅在部分参数或全部参数上更新（LoRA、Fine-tune、PT）**
   常见方式包括全参数微调、LoRA 微调、Prefix-Tuning 等。

4. **目标是让模型在特定任务上效果更好**
   例如提高客服机器人命中率、让模型生成符合企业风格的内容、增强模型在专业领域的回答能力。

5. **本质是“定制化”**
   让一个通用大模型变成一个业务专家模型。


## 2、大模型中的“SFT”是什么？

SFT 的全称是 **Supervised Fine-Tuning（监督式微调）**，是大模型训练流程中最关键、最常用的一步。

**SFT 是大模型在预训练之后，使用人工标注的高质量示例进行的“监督式微调”。**

它的目标是让模型按照人类意图输出更自然、更符合任务要求的回答。

要点包括：

1. **有监督学习**：使用成对的“输入 → 标准答案”数据（prompt、response）。
2. **让模型学会“该怎么回答”**，从通用模型变成任务模型。
3. **典型用于对话、指令跟随、任务执行等场景**。
4. **常作为 RLHF（强化学习）之前的重要步骤**。

一句话总结：

**SFT 就是用人工示范样例教大模型如何按人类的方式完成任务，是大多数指令模型（如 ChatGPT 类模型）的核心训练步骤。**

## 3、大模型中的“CoT（Chain of Thought）”是什么？

**CoT 是 Chain of Thought 的缩写，指“思维链”提示方式。**

通过让模型输出中间推理过程，而不是直接给答案，可以显著提升模型在**复杂推理、数学、逻辑、规划**等任务上的表现。

关键点：

1. **本质是显式推理**
   让模型把每一个推理步骤写出来，再得到最终结论。

2. **依赖示例（Few-shot CoT）**
   给模型提供带“步骤和答案”的示例，它就会模仿这种推理模式。

3. **提升模型推理能力**
   特别是在数学计算、逻辑推断、程序题、规划路径等链式推理任务中效果很好。

4. **不是 RLHF 或 SFT 的替代，而是 Prompt 技术**
   CoT 可用于推理测试，也常作为数据加入 SFT，让模型具备推理能力。

一句话总结：

**CoT 是一种让模型“说出思考过程”的方法，通过显式推理链条来提升模型的逻辑和推理效果。**

## 4、大模型中的“RAG（Retrieval-Augmented Generation）”是什么

**RAG 是 Retrieval-Augmented Generation，用外部检索系统补充大模型的知识，让模型“先查再答”。**
解决大模型幻觉、知识过时、领域知识不足的问题。

核心流程包含三步：

1. **检索（Retrieval）**
   根据用户问题，从向量数据库或文档库中召回最相关的内容（Embedding 相似度）。

2. **增强（Augment）**
   将检索到的内容与原问题拼接，作为模型新的输入 Prompt。

3. **生成（Generation）**
   大模型基于“问题 + 外部知识”生成更加准确、可引用的回答。

典型适用场景：

* 企业知识库问答
* 文档助手/法律助手/医疗助手
* FAQ 自动化
* 专业领域问答
* 需要实时信息的任务（例如最新文档、公司内部资料）

一句话总结：
**RAG 让模型不依赖记忆，而是通过检索获得事实，确保回答更稳定、更准确、更可控。**

## 5、大模型中的“RL（Reinforcement Learning）”是什么？

**RL 是 Reinforcement Learning 的缩写，即强化学习。**
在大模型训练中，它是让模型通过“奖励函数”不断调整策略，从而更符合人类意图的一种训练方式。

关键特点：

1. **通过奖励优化模型行为**
   不再依赖标准答案，而是通过“好行为给奖励、坏行为给惩罚”来优化模型的输出策略。

2. **常用于人类偏好对齐（Alignment）**
   典型流程是人类先给出模型回答的偏好排序，然后训练一个奖励模型，再用 RL（如 PPO）优化模型。

3. **解决 SFT 的不足**
   SFT 只能让模型“模仿人类示例”，RL 可以进一步强化模型应该做的行为，弱化不希望的行为。

4. **用于提升模型的礼貌性、安全性、稳定性和可控性**
   例如减少幻觉、避免敏感内容、提升回答一致性。

一句话总结：
**RL 是让大模型通过奖励机制学习如何“更像人类希望的那样”回答，是对齐技术中非常关键的一步。**

## 6、大模型中的“GRPO（Generative Rejection Preference Optimization）”是什么？


**GRPO 是 Generative Rejection Preference Optimization，是一种不依赖复杂强化学习（如 PPO），通过“生成-拒绝-偏好优化”来对齐大模型的训练技术。**

核心思想是：

1. **模型生成多个候选回答**
   例如针对一个 prompt 生成 K 个不同的输出。

2. **利用规则、奖励模型或过滤逻辑对回答进行排序或淘汰**
   识别出“好的回答”和“差的回答”。

3. **只保留高质量的人类偏好回答，用监督方式继续训练模型**
   不做真正的 RL，不需要 PPO，也不需要复杂的环境交互。

4. **实现“更简单的偏好优化”**
   既保留了 RLHF“偏好对齐”的优势，又避免 RL 的高成本和不稳定性。

一句话总结：
**GRPO 是一种“用拒绝差答案 + 继续监督训练”来对齐大模型偏好的方法，被认为是比传统 RLHF 更稳、更便宜的下一代偏好优化技术。**

## 7、大模型中的“MOE（Mixture of Experts）”是什么？


**MOE 是 Mixture of Experts，即“专家混合模型”。它通过引入多个专家子模型，并使用路由器在每次前向计算中只激活少数专家，从而实现“大容量、低计算”的模型结构。**

核心机制：

1. **多个专家（Expert）并行存在**
   每个 expert 是一组独立的全连接层或 FFN。

2. **路由器（Router/Gating Network）选择性激活专家**
   输入 token 经过 Router 后，会被分配到最相关的少量专家（如 top-1 或 top-2）。

3. **稀疏激活（Sparse Activation）**
   虽然模型包含成百上千个专家，但每次只使用 1-2 个，因此计算量大幅降低。

4. **达到“参数多，算力少”的效果**
   例如参数量看起来 500B，但训练/推理的 FLOPs 类似 50B。

一句话总结：
**MOE 用“专家路由 + 稀疏激活”实现了超大参数规模与高效计算的统一，是目前扩展大模型能力的重要架构。**

## 8、大模型中的“Scaling（扩展规律）”是什么？

**Scaling（扩展规律）是指：大模型的损失函数、能力和泛化性能，随着参数规模、训练数据量和计算量按特定比例扩大时，会呈现近似幂律（Power Law）的规律性提升。**

也就是说，只要给模型：

* **更多参数（更大的网络）**
* **更多训练数据**
* **更多计算**

其性能就会按照可预测的数学曲线持续提升。

典型特征：

1. **呈幂律关系（Power-Law）**
   损失 L 与规模 N 满足公式：
   [
   L(N) = aN^{-b} + c
   ]

2. **参数、数据、算力必须协同扩展**
   不能只扩大参数或数据，否则会浪费算力。

3. **可根据扩展规律规划下一代模型规模**
   例如 OpenAI、DeepMind、Anthropic 都依赖 scaling law 设计 GPT、Gemini、Claude。

一句话总结：
**Scaling Law 让大模型从“靠经验调参”变成“按数学公式设计模型”，并推动了 GPT-3 以后的整个大模型时代。**

## 9、大模型中的“软标签（Soft Label）”是什么？


**软标签（Soft Label）是指：标签不再是单一的确定类别，而是一个概率分布，用来表达模型或数据对各类别的“置信度”。**
常用于知识蒸馏、偏好建模和对齐训练，让模型学习“更细腻、更接近人类判断”的信息。

与传统 **硬标签（Hard Label）** 的区别：

* **硬标签**：只有一个正确答案，例如 `[0,0,1,0]`
* **软标签**：是一组概率，例如 `[0.1, 0.2, 0.6, 0.1]`

---

### 为什么大模型要用软标签？

1. **包含更多信息**
   不是告诉模型“谁是正确的”，而是告诉模型“每个答案有多合理”。

2. **提升训练稳定性**
   比硬标签更平滑，减少梯度震荡。

3. **知识蒸馏（Distillation）核心机制**
   学生模型学习老师模型的“概率分布”，而不是死记一个答案。

4. **偏好优化（Preference Optimization）中非常常用**
   例如在 RM（奖励模型）训练中，软标签可以表达不同回答之间的微弱偏好差异。

5. **模型输出更自然、逼近人类评判**
   尤其在开放式问答场景，答案通常并非绝对唯一。

---

### 一句话总结

**软标签就是“概率形式的标签”，不仅告诉模型哪个答案正确，还告诉模型每个答案正确的程度，用于更精细、更稳定的训练。**

## 10、大模型中的“噪声（Noise）”是什么？



**大模型中的“噪声（Noise）”指的是训练过程中人为加入或不可避免存在的随机扰动，用来提升模型的泛化能力、稳定性以及训练多样性。**

噪声的来源可以是随机初始化、dropout、采样温度、梯度噪声、RL 中的探索噪声等。

它们共同作用于：

* 避免过拟合
* 提高模型鲁棒性
* 增强生成多样性
* 支持 RL 或偏好优化中的探索

---

### 大模型中的常见噪声类型（面试加分）

1. **数据噪声**
   来源于不干净的训练数据，如拼写错误、语法缺失、事实模糊等。

2. **模型训练噪声**

    * 随机初始化权重
    * Dropout
    * Batch shuffle
    * 梯度噪声（SGD 本身就是 noisy gradient）

3. **生成阶段噪声**

    * Sampling randomness（采样温度、top-k、top-p）
    * 引导输出多样性，避免 deterministic 输出

4. **RL / 偏好训练中的噪声**

    * Exploration noise（策略探索时的随机性）
    * 通过噪声避免策略 collapse
    * 在 DPO/RLHF 中用于提高样本多样性

5. **对齐与蒸馏训练中的噪声**

    * Soft labels 提供的概率不确定性也可视为“软噪声”
    * 使训练更平滑、更稳定

---

### 一句话总结（面试最重要）

**噪声在大模型中不是坏事，而是增强泛化能力、提高稳定性、增加多样性和支持策略探索的核心机制。**

## 11、大模型中的“温度（Temperature）”是什么？

**Temperature（温度）是大模型生成阶段控制“随机性”和“多样性”的参数。**

作用可以一句话概括：

**温度越高，输出越随机、多样；温度越低，输出越确定、保守。**

典型规律：

* **T = 1**：默认随机度
* **T < 1**：更确定，更像“考试模式”
* **T > 1**：更发散，更有创造性

它通过对 logits 做缩放：
[
p_i = \frac{e^{logit_i / T}}{\sum_j e^{logit_j / T}}
]

所以：
**Temperature 是一个控制生成“创造性”与“确定性”的关键调节器。**


## 12、大模型中的“对齐（Alignment）”是什么

**对齐（Alignment）指的是让大模型的行为“符合人类意图、安全要求与价值偏好”的一整套技术体系。**

一句话概括：
**Alignment 让模型不只是“能生成内容”，而是“按人类希望的方式生成内容”。**

核心目标包括：

1. **按人类意图回答**（Instruction following）
2. **避免危险、有害、不当内容**（Safety）
3. **符合人类价值观与偏好**（Human preference）
4. **让输出可靠、稳健、可控**（Robustness & Control）

主流技术手段：

* **SFT（监督微调）**：教模型“怎么回答”
* **Reward Model / Preference Model（奖励模型）**：学习人类的好坏判断
* **RLHF / DPO / GRPO 等偏好优化**：让模型更符合人类偏好
* **安全过滤与拒答机制**
* **对外部知识/事实的约束（如 RAG）**

## 13、大模型中的“上下文窗口（Context Window）”是什么？

**上下文窗口（Context Window）指的是大模型一次能够接收并参与推理的输入 token 的最大范围。**

一句话概括：
**Context Window 就是模型“短期记忆”的容量，决定它一次能看多长的内容。**

核心点：

1. **由 token 数量决定（不是字数）**
   例如 4K、16K、32K、128K、1M tokens。

2. **超出窗口的内容会被截断或遗忘**
   模型无法在推理时访问窗口外的信息。

3. **上下文越大，能处理的任务越复杂**
   如长文阅读、代码库理解、大型合同分析等。

4. **不等于模型参数，不等于长期记忆**
   只是一次推理的“可见区域”。

一句话总结：
**上下文窗口就是大模型单次推理能读进“脑子里”的最大内容大小，窗口越大，能处理的长文本任务越强。**

## 14、大模型中的“泛化（Generalization）”是什么？

**泛化（Generalization）指的是大模型在训练数据之外，依然能够表现良好、正确回答新问题的能力。**

一句话概括：
**泛化就是模型“举一反三”的能力，而不是只会背训练数据。**

核心点：

1. **不依赖记忆训练数据**
   能在未见过的新输入上给出合理回答。

2. **体现模型真实能力，而非过拟合结果**
   泛化强的模型能推理、类比、迁移知识。

3. **是评估大模型质量的关键指标**
   例如数学题推理、逻辑题、跨领域问答等。

4. **由模型规模、数据质量、训练方式共同决定**
   高质量多样数据、合理正则化、扩展规律都能提升泛化。

一句话总结：
**泛化是大模型“面对没见过的问题也能给出好答案”的能力，是衡量模型智慧而非死记硬背的核心指标。**

## 15、大模型与传统机器学习模型的区别是什么？

**1. 规模与数据量不同**
大模型基于超大参数量和超大规模语料预训练；传统模型规模小、依赖有限的标注数据。

**2. 训练范式不同**
大模型采取“预训练 + 微调”范式，先学通用知识再适配任务；
传统模型是“单任务监督学习”，每种任务都要单独训练。

**3. 特征方式不同**
大模型自动从海量数据中学习特征；
传统模型依赖人工特征工程。

**4. 能力范围不同**
大模型具备通用理解、生成、推理等多任务能力；
传统模型只能解决特定目标，如分类、回归等单一任务。

**一句话总结：**
**大模型是通用、多任务、自学习的智能模型；传统机器学习模型是专用、单任务、依赖人工特征的传统算法。**


## 16、什么是 Transformer 架构？

**Transformer 是一种基于“自注意力机制（Self-Attention）”的深度学习架构，用来高效处理序列数据（文本、语音、代码等）。**
它由 Google 在 2017 年提出，已成为现代大模型（GPT、BERT、Claude、Gemini 等）的基础结构。

### Transformer 的核心特点

**1. 基于自注意力（Self-Attention）而非 RNN/CNN**
通过注意力机制，模型能直接“全局看”整个序列，不依赖逐步递归，因此速度快、并行性强。

**2. 结构由 Encoder 和 Decoder 组成**

* Encoder 用于理解输入
* Decoder 用于生成输出
  GPT 只用 Decoder；BERT 只用 Encoder。

**3. 完全并行化训练**
相比 RNN 不需要一步步处理序列，大幅提升训练效率。

**4. 能捕捉长距离依赖**
自注意力机制可以直接关注文本中任意位置的内容，长文本建模能力强。

---

### Transformer 的关键模块

* **Self-Attention（自注意力）**：核心机制
* **Multi-Head Attention（多头注意力）**：并行关注不同特征
* **Feed-Forward Network（前馈层）**
* **Residual + LayerNorm（残差结构）**
* **Positional Encoding（位置编码）**：补充序列顺序信息

---

### 一句话总结（面试高频）

**Transformer 是一种靠自注意力机制进行并行序列建模的架构，具有强大的长距依赖建模能力，是现代大模型的核心基础。**

## 17、为什么 Transformer 能取代 RNN？

### Transformer 能取代 RNN 的核心原因

**1. 并行计算能力强（最关键）**
RNN 必须序列逐步处理，无法并行；
Transformer 的自注意力机制可一次处理整段文本，训练速度提升数十倍。

**2. 长距离依赖建模能力强**
RNN 的记忆会随时间步衰减；
Transformer 的 Self-Attention 可以直接关注任意距离的词，长文本效果显著更好。

**3. 表达能力更强**
多头注意力可以同时从多个角度理解输入，比 RNN 的单一隐状态更丰富。

**4. 优化更稳定**
RNN 训练容易梯度消失/爆炸；
Transformer 配合残差结构和 LayerNorm，训练更稳定、容易扩大规模。

**5. 更适合大规模预训练**
Transformer 可以高效利用海量数据，而 RNN 扩展到百亿参数几乎不可能。

---

### 一句话总结

**Transformer 之所以能取代 RNN，是因为它能并行训练、能处理长依赖、表达能力更强、可大规模扩展，是现代大模型的最优架构。**

## 18、什么是预训练？

**预训练（Pre-training）指的是先用海量、通用的数据让模型学习语言规律和基础知识，为之后的下游任务打好通用能力基础。**

### 什么是预训练？

**预训练是让模型在大规模未标注语料上，通过自监督方式学习通用的语言模式、知识和推理能力的过程。**

关键点：

1. **数据规模大**：通常是互联网级别的海量语料。
2. **无需人工标注**：通过自监督任务（如预测下一个词）学习结构。
3. **目标是获得通用能力**：理解、生成、世界知识等。
4. **为后续任务打基础**：后续通过 SFT、RLHF、微调适配具体场景。

---

### 一句话总结

**预训练就是先用海量数据让模型“学会语言和通用知识”，再在此基础上做特定任务的微调。**

## 19、什么是参数量，比如7B、13B是什么意思？

**参数量指的是模型中可训练参数（权重）的总数量。**
它反映了模型的规模和容量。

像 **7B、13B** 这样的写法是业界的标准表示：

* **7B = 7 Billion = 70 亿参数**
* **13B = 13 Billion = 130 亿参数**

参数越多，模型的表示能力、推理能力和知识容量往往越强，但训练和推理成本也越高。

---

### 一句话总结

**7B、13B 是模型规模的代号，表示模型拥有 70 亿、130 亿个可训练参数。参数量越大，模型通常越强，也越耗算力。**

## 20、什么是 Prompt？为什么提示词能影响模型回答

### 什么是 Prompt？

**Prompt（提示词）就是输入给大模型的指令或表达方式，用来告诉模型你希望它执行什么任务。**
它包含问题本身、任务描述、上下文、格式要求等。

### 为什么 Prompt 能影响模型回答？

**因为大模型是条件生成模型，它输出的内容完全取决于输入（Prompt）提供的语义线索和意图信息。**

具体原因：

1. **大模型是在预测“下一步最可能的词”**
   不同提示会引导模型选择不同的生成路径。

2. **Prompt 会改变模型对任务的理解方式**
   描述清楚任务、格式、角色，会让模型沿着更明确的分布生成。

3. **训练过程中模型学习了“根据指令完成任务”的模式**
   像 SFT、RLHF 等步骤会强化模型对 Prompt 的响应方式。

4. **Prompt 中的关键字、结构、语气会影响注意力分布**
   从而影响模型对哪些信息“更关注”。

---

### 一句话总结

**Prompt 是你给模型的指令；模型根据 Prompt 决定理解方式和生成方向，所以提示词不同，回答会发生显著变化。**

## 21、什么是 Embedding？

### 什么是 Embedding？

**Embedding 是把文本、单词或句子转换成一个稠密的向量表示，用来让模型以数学方式理解语义。**

特点：

1. **向量化**：把文本变成一串数字（如 768 维、1024 维）。
2. **语义相似即向量接近**：相似句子→向量距离近；不同语义→距离远。
3. **用途广泛**：搜索、推荐、聚类、RAG 检索、相似度计算等。

---

### 一句话总结

**Embedding 是“文本的语义向量表示”，让机器能用数学方式理解和比较语言。**


## 22、什么是 Attention 机制？


### 什么是 Attention 机制？

**Attention（注意力机制）是让模型在处理序列时，根据相关性动态分配“关注程度”的方法。**
也就是说，模型会自动判断输入中哪些词对当前词最重要，并给予更高权重。

---

### 关键作用

1. **突出重点信息**：重要词获得高注意力，不重要的词被弱化。
2. **建模长距离依赖**：可以直接关注序列中任意位置的内容。
3. **提升表达能力**：多头注意力能从多个视角理解语义。

---

### 一句话总结

**Attention 机制让模型在处理文本时“关注更重要的部分”，是 Transformer 能理解长语义和上下文的核心。**

## 23、什么是 Self-Attention？


### 什么是 Self-Attention？

**Self-Attention（自注意力）是让序列中的每个位置，自动“关注”同一序列中其他位置的重要程度，从而动态计算当前词的表示。**

换句话说：

**一个词在理解自己时，会参考句子中所有其他词，并根据相关性分配权重。**

---

### 为什么重要？

1. **捕捉长距离依赖**（能跨句看上下文）
2. **并行计算**（相比 RNN 不需要按顺序）
3. **表达能力强**（不同词之间的关系可直接建模）

---

### 一句话总结

**Self-Attention 是让句子中的每个词根据上下文动态计算“该关注谁”的机制，是 Transformer 的核心能力来源。**

## 24、为什么要使用多头注意力（Multi-head Attention）？


### 为什么要使用多头注意力（Multi-head Attention）？

**因为单一的注意力头只能关注一种关联特征，而多头注意力可以让模型从多个角度、多个子空间同时理解语义。**

核心原因：

1. **多视角表达能力更强**
   每个注意力头学习不同的关系：
   例如一个头关注主谓关系，一个头关注指代，一个头关注长距依赖。

2. **分解注意力空间，提升表示丰富度**
   通过拆分为多个低维子空间，模型能捕捉更细粒度的信息结构。

3. **提高模型稳定性和泛化能力**
   多个头的结果被整合，减少单头注意力的偏差。

4. **并行计算，不增加序列处理瓶颈**
   多个头可同时计算，不影响整体效率。

---

### 一句话总结

**多头注意力能让模型从多个角度同时理解句子，使表达能力更强、语义捕捉更全面，这是 Transformer 性能大幅优于 RNN 的关键机制之一。**

## 25、什么是LangChain？

**LangChain 是一个用于构建基于大模型（LLM）的应用开发框架。**
它把调用大模型需要的各种能力模块化，让开发者更容易构建复杂的 AI 应用。

### 什么是 LangChain？

**LangChain 是一个围绕大模型的应用开发框架，用于将 LLM 与外部工具、数据源、知识库等组合起来，构建具有“记忆、检索、推理、工具调用”等能力的应用。**

---

### 它主要解决什么问题？

1. **对接大模型**（OpenAI、ChatGPT、LLaMA 等）
2. **管理 Prompt**（Prompt 模板、动态拼接）
3. **外部知识检索（RAG）**
4. **工具调用**（搜索、数据库、网络请求等）
5. **流程编排（Chains / Agents）**
6. **记忆管理（Memory）**

---

### 为什么有用？

因为实际 AI 应用不仅仅是“问一句 → 回一句”，而是需要：

* 用数据库查信息
* 检索文档
* 多步骤推理
* 访问 API
* 保持对话上下文
* 调用外部工具执行操作

LangChain 把这些能力封装好，用“链式流程”和“代理（Agent）”形式组合起来，极大降低开发难度。

---

### 一句话总结

**LangChain 是一个帮助开发者快速构建大模型应用的框架，通过链式处理、工具调用和知识检索，让 LLM 从纯对话变成“可行动的智能体”。**


## 26、什么是向量数据库

**向量数据库（Vector Database）是一种专门用于存储、管理和高效检索“向量 Embedding”的数据库。**
它不是存文本，而是存“文本的语义向量”，用于相似度搜索。

### 什么是向量数据库？

**向量数据库是用于存储 Embedding 向量，并通过相似度搜索（如余弦距离、欧氏距离）来快速找到“语义最相似内容”的数据库。**

它的核心作用是：
**给定一个查询向量，找到最相似的向量记录。**

---

### 为什么需要向量数据库？

因为大模型使用 Embedding 表示语义：

* 语义相似 → 向量距离近
* 语义不同 → 距离远

向量数据库能在百万、亿级向量中快速查找相似内容，传统数据库做不到。

---

### 主要用途

* **RAG 检索增强**
* **语义搜索**
* **文本、代码、图片相似度匹配**
* **推荐系统**
* **知识库问答**

典型产品：Milvus、Faiss、Pinecone、Weaviate、Chroma。

---

### 一句话总结

**向量数据库是“存语义向量、按相似度搜索”的数据库，是 RAG 与现代 AI 应用的核心基础设施。**

## 27、什么是MCP（大模型上下文协议）？

### 什么是 MCP（Model Context Protocol，大模型上下文协议）？

**MCP 是一种让大模型能够安全、标准化地访问外部数据源、工具和系统的通信协议。**
它定义了模型与外部世界交互的统一接口，使模型能够在受控环境中**调用工具、读取数据、执行操作**，从而具备真实的行动能力。

---

### MCP 的核心作用

1. **统一标准**
   让不同模型、不同工具之间“说同一种语言”。

2. **安全访问外部数据**
   通过严格权限控制，让模型只能访问被授权的数据或 API。

3. **让模型具备“工具使用能力”**
   MCP 提供标准化的 Tool 调用机制，让模型能执行：

* 查询数据库
* 调用 HTTP 服务
* 读取文件
* 运行内部 API
* 与企业系统交互

4. **实现上下文扩展（Context Augmentation）**
   模型可以通过 MCP 动态获取更多上下文，而不是只依靠输入文本。

---

### MCP 解决了什么问题？

* 模型无法直接访问外部系统
* 工具调用缺乏统一标准
* 企业数据接入困难
* 安全权限难以管理
* 不同 LLM 厂商生态割裂

---

### 一句话总结

**MCP 是让大模型能“以统一、安全的方式接入外部工具和数据”的协议，是工具调用和智能体能力的基础标准。**

## 28、Zero-shot、One-shot、Few-shot 的区别

### Zero-shot、One-shot、Few-shot 的区别

**1. Zero-shot（零样本）**
模型在**没有任何示例**的情况下，直接根据指令完成任务。
例：直接问“总结这段文字”。

**2. One-shot（单样本）**
给模型 **一个示例** 作为参考，再让它完成同类型任务。
例：先给一个“示范总结”，再让模型总结另一段。

**3. Few-shot（少样本）**
给模型 **多个示例（通常 2–10 个）**，让它更准确地理解任务模式，再执行任务。
例：提供多个“问题→答案”的示例，提升模型执行质量。

---

### 一句话总结

**Zero-shot 完全靠指令，One-shot 给一个示例，Few-shot 给多个示例，让模型通过示例学习任务格式与规律。**

## 29、什么是 Prompt Engineering？

**Prompt Engineering 是设计和优化提示词，让大模型按预期产生更准确、可靠输出的技术。**

### 什么是 Prompt Engineering？

**Prompt Engineering（提示工程）是通过精心设计输入指令，让大模型更好地理解任务、遵循格式、减少误解，从而得到更理想输出的过程。**

它的核心目标是：
**让模型“按你想要的方式”工作。**

---

### 为什么需要 Prompt Engineering？

因为大模型的输出质量依赖输入表达：

* 指令越清晰，结果越稳定
* 上下文越完整，回答越准确
* 示例越合理，行为越可控

---

### 常见方法

* 明确任务描述
* 指定输出格式（JSON、列表等）
* 使用 few-shot 示例
* 使用角色扮演（如“你是一个资深面试官”）
* 分解任务（Chain-of-Thought）
* 约束规则与边界

---

### 一句话总结

**Prompt Engineering 就是“让模型听懂你的话”的技术，通过设计提示词来引导模型正确、稳定、高质量地完成任务。**

## 30、什么是MoE架构？

### 什么是 MoE 架构？

**MoE（Mixture of Experts，专家混合架构）是一种让模型由多个“专家子网络”组成，并根据输入动态选择少数专家参与计算的模型结构。**

换句话说：

**模型很大，但每次只激活一小部分专家，因此具备“大容量、低计算”的优势。**

---

### MoE 的核心机制

1. **多个专家（Expert Networks）并行存在**
   每个 Expert 是一套独立的 FFN 子网络。

2. **路由器（Router/Gating Network）负责调度**
   根据输入 token，选择 Top-1 或 Top-2 专家来处理。

3. **稀疏激活（Sparse Activation）**
   虽然模型可能有数百亿参数，但每次计算只激活少量，从而降低成本。

---

### 为什么 MoE 重要？

* **大幅提升模型容量（参数量更大）**
* **计算量不等比例增加（成本几乎不变）**
* **多任务、多领域扩展能力强**
* **是 DeepSeek-MOE、Mixtral 等高性能模型的核心技术**

---

### 一句话总结

**MoE 架构通过“专家路由 + 稀疏激活”实现超大参数规模与低计算成本，是现代大模型扩展能力的重要架构之一。**

## 31、什么是 Self-RAG？


### 什么是 Self-RAG？

**Self-RAG 是一种让大模型“自我驱动检索”的 RAG 方法，即模型不仅生成答案，还能主动决定何时检索、检索什么，以及如何使用检索结果。**

与传统 RAG 的区别在于：

* **传统 RAG：**由外部系统固定地执行检索，再把文档给模型
* **Self-RAG：**模型自己控制检索流程，具备“自我查询”和“自我验证”能力

---

### Self-RAG 的核心能力

1. **模型主动判断是否需要检索**
   不是每个问题都检索，减少冗余查询。

2. **模型自己生成检索查询（Query）**
   提升知识召回的准确性。

3. **模型对检索结果进行自我评估**
   判断结果是否可信、是否需要重新检索。

4. **模型能自我迭代答案**
   发现不充分时重新召回资料并修正回答。

---

### 一句话总结

**Self-RAG 是让模型“自己决定检索、自己写查询、自己评估结果”的增强版 RAG，使生成过程更智能、更自主、更可靠。**

## 32、RAG 能解决哪些大模型问题？

### RAG 能解决哪些大模型问题？

**1. 解决“知识过时”问题**
大模型训练数据有时间截止，RAG 可实时检索最新信息。

**2. 解决“幻觉（Hallucination）”问题**
通过引用真实文档，让模型基于事实回答，而不是编造内容。

**3. 解决“领域知识不足”问题**
把企业资料、专业文档纳入检索范围，让模型具备行业知识。

**4. 解决“上下文不够”问题**
模型上下文窗口有限，RAG 可动态把外部知识补充进去。

**5. 解决“数据隐私”问题**
企业内部数据可以在本地向量库中检索，不暴露给大模型。

**6. 提升可控性与可解释性**
模型回答依赖检索文档，可追溯来源，比纯生成更可控。

---

### 一句话总结

**RAG 解决大模型“知识不全、知识过时、幻觉严重、行业不足、不可控”这些核心问题，让模型从“会说”变成“说得准”。**

## 33、推理延迟和 Token 数量的关系？

### 推理延迟和 Token 数量的关系

**推理延迟与 Token 数量基本呈线性关系：输入 Token 越多、输出 Token 越多，推理时间越长。**

可以拆成两部分：

1. **输入 Token（Prompt 长度）**

    * Transformer 在每个解码步骤都要重新读取全部输入
    * 输入越长，每步解码成本越大
      **延迟 ≈ O(N_input)**

2. **输出 Token（模型生成长度）**

    * 每生成一个 Token，都需要执行一次完整的解码计算
    * 输出越长，总延迟越高
      **延迟 ≈ O(N_output)**

因此推理总时长通常近似为：

[
\text{Latency} \propto N_{\text{input}} + N_{\text{output}}
]

---

### 一句话总结

**Token 越多，推理越慢；输入越长、生成越长，延迟几乎线性增加。**
